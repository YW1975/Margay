/**
 * Model Evaluation Runner v2.0
 * - Reads model-manifest.json (generated by resolve-models.ts)
 * - Falls back to inline buildModelConfigs() if no manifest exists (v1.0 compat)
 * - Supports multi-turn prompts (M3) via messages[] array
 * - Gemini: via @margay/agent-core engine
 * - Others: via OpenAI-compatible API (direct fetch)
 *
 * Usage:
 *   npx tsx scripts/model-eval/resolve-models.ts   # Run preflight first
 *   npx tsx scripts/model-eval/run-eval.ts          # Then run eval
 *
 * Reads API keys from models_test.key in project root.
 * Results written incrementally to scripts/model-eval/results.json
 * Final report generated at docs/model-eval-report.md
 */

import path from 'node:path';
import os from 'node:os';
import fs from 'node:fs';
import { EVAL_PROMPTS, PROMPT_SET_VERSION, type EvalPrompt, type ChatMessage } from './prompts';

const PROJECT_ROOT = path.resolve(__dirname, '../..');
const CORE_PATH = path.resolve(PROJECT_ROOT, 'vendor/gemini-cli/packages/core/dist/index.js');
const RESULTS_FILE = path.resolve(__dirname, 'results.json');
const REPORT_FILE = path.resolve(PROJECT_ROOT, 'docs/model-eval-report.md');
const KEY_FILE = path.resolve(PROJECT_ROOT, 'models_test.key');
const MANIFEST_FILE = path.resolve(__dirname, 'model-manifest.json');

const PER_MODEL_TIMEOUT_MS = 60_000;
const COST_CAP_USD = 15;

interface ModelConfig {
  provider: string;
  model: string;
  mode: 'gemini-engine' | 'openai-api';
  apiKey: string;
  baseUrl: string;
}

interface EvalResult {
  provider: string;
  model: string;
  promptId: string;
  status: 'pass' | 'fail' | 'error' | 'timeout' | 'skip';
  response: string;
  latencyMs: number;
  error?: string;
  keywordsMatched?: number;
  keywordsTotal?: number;
}

interface EvalSession {
  version: string;
  startedAt: string;
  results: EvalResult[];
  skippedProviders: string[];
}

// --- Parse key file ---
function loadKeys(): Record<string, string> {
  const content = fs.readFileSync(KEY_FILE, 'utf-8');
  const keys: Record<string, string> = {};
  for (const line of content.split('\n')) {
    const trimmed = line.trim();
    if (!trimmed || trimmed.startsWith('#')) continue;
    const cleanLine = trimmed.replace(/^\(.*?\)/, '').trim();
    if (!cleanLine) continue;
    const colonIdx = cleanLine.indexOf(':');
    if (colonIdx <= 0) continue;
    const provider = cleanLine.slice(0, colonIdx).trim().toLowerCase();
    const key = cleanLine.slice(colonIdx + 1).trim();
    if (key) keys[provider] = key;
  }
  return keys;
}

// --- Load configs from manifest (v2.0) ---
function loadFromManifest(keys: Record<string, string>): ModelConfig[] | null {
  if (!fs.existsSync(MANIFEST_FILE)) return null;

  const manifest = JSON.parse(fs.readFileSync(MANIFEST_FILE, 'utf-8'));
  const configs: ModelConfig[] = [];

  for (const entry of manifest.models) {
    if (entry.status === 'skip') continue;

    const apiKey = keys[entry.keyName];
    if (!apiKey) continue;

    configs.push({
      provider: `${entry.provider}${entry.status === 'fallback' ? ' (fb)' : ''}`,
      model: entry.resolvedModel,
      mode: entry.mode,
      apiKey,
      baseUrl: entry.baseUrl,
    });
  }

  console.log(`Loaded ${configs.length} models from manifest v${manifest.version}`);
  console.log(`  Resolved at: ${manifest.resolvedAt}`);
  const skipped = manifest.models.filter((m: any) => m.status === 'skip');
  if (skipped.length > 0) {
    console.log(`  Skipped: ${skipped.map((m: any) => `${m.provider} (${m.skipReason})`).join(', ')}`);
  }
  return configs;
}

// --- Inline model configs (v1.0 fallback) ---
function buildModelConfigs(keys: Record<string, string>): ModelConfig[] {
  console.log('WARNING: No model-manifest.json found. Using v1.0 inline configs.');
  console.log('Run "npx tsx scripts/model-eval/resolve-models.ts" to generate manifest.\n');

  const configs: ModelConfig[] = [];

  if (keys['gemini']) {
    configs.push({
      provider: 'Google',
      model: 'gemini-2.5-flash',
      mode: 'gemini-engine',
      apiKey: keys['gemini'],
      baseUrl: '',
    });
  }

  if (keys['openai']) {
    configs.push({
      provider: 'OpenAI',
      model: 'gpt-4.1-mini',
      mode: 'openai-api',
      apiKey: keys['openai'],
      baseUrl: 'https://api.openai.com/v1',
    });
  }

  if (keys['deepseek']) {
    configs.push({
      provider: 'DeepSeek',
      model: 'deepseek-chat',
      mode: 'openai-api',
      apiKey: keys['deepseek'],
      baseUrl: 'https://api.deepseek.com',
    });
  }

  if (keys['minimax']) {
    configs.push({
      provider: 'MiniMax',
      model: 'MiniMax-M1',
      mode: 'openai-api',
      apiKey: keys['minimax'],
      baseUrl: 'https://api.minimax.chat/v1',
    });
  }

  const orKey = keys['openrouter'];
  if (orKey) {
    for (const { provider, model } of [
      { provider: 'Anthropic (OR)', model: 'anthropic/claude-sonnet-4' },
      { provider: 'Qwen (OR)', model: 'qwen/qwen3-max' },
      { provider: 'Zhipu (OR)', model: 'z-ai/glm-5' },
      { provider: 'Kimi (OR)', model: 'moonshotai/kimi-k2.5' },
      { provider: 'Baidu (OR)', model: 'baidu/ernie-4.5-300b-a47b' },
    ]) {
      configs.push({
        provider,
        model,
        mode: 'openai-api',
        apiKey: orKey,
        baseUrl: 'https://openrouter.ai/api/v1',
      });
    }
  }

  return configs;
}

// --- Build messages for a prompt ---
function buildMessages(prompt: EvalPrompt): ChatMessage[] {
  if (prompt.messages && prompt.messages.length > 0) {
    return prompt.messages;
  }
  return [{ role: 'user', content: prompt.prompt }];
}

// --- OpenAI-compatible API call ---
async function callOpenAIApi(
  config: ModelConfig,
  prompt: EvalPrompt,
): Promise<{ text: string; error?: string }> {
  const controller = new AbortController();
  const timeoutId = setTimeout(() => controller.abort(), PER_MODEL_TIMEOUT_MS);

  const messages = buildMessages(prompt);

  try {
    const resp = await fetch(`${config.baseUrl}/chat/completions`, {
      method: 'POST',
      headers: {
        'Content-Type': 'application/json',
        Authorization: `Bearer ${config.apiKey}`,
        ...(config.baseUrl.includes('openrouter')
          ? {
              'HTTP-Referer': 'https://margay.app',
              'X-Title': 'Margay Model Eval',
            }
          : {}),
      },
      body: JSON.stringify({
        model: config.model,
        messages,
        max_tokens: prompt.maxTokens || 1024,
      }),
      signal: controller.signal,
    });

    if (!resp.ok) {
      const errBody = await resp.text().catch(() => '');
      return { text: '', error: `HTTP ${resp.status}: ${errBody.slice(0, 200)}` };
    }

    const data = (await resp.json()) as any;
    const text = data.choices?.[0]?.message?.content || '';
    return { text };
  } catch (err: any) {
    if (err.name === 'AbortError') return { text: '', error: 'timeout' };
    return { text: '', error: err.message?.slice(0, 200) };
  } finally {
    clearTimeout(timeoutId);
  }
}

// --- Gemini engine call ---
async function callGeminiEngine(
  core: any,
  config: ModelConfig,
  prompt: EvalPrompt,
): Promise<{ text: string; error?: string }> {
  for (const k of ['GEMINI_API_KEY', 'GOOGLE_GEMINI_BASE_URL', 'OPENAI_API_KEY', 'OPENAI_BASE_URL']) {
    delete process.env[k];
  }
  process.env.GEMINI_API_KEY = config.apiKey;

  const workspace = os.tmpdir();
  const coreConfig = new core.Config({
    sessionId: `eval-${Date.now()}`,
    targetDir: workspace,
    cwd: workspace,
    debugMode: false,
    model: config.model,
    usageStatisticsEnabled: false,
    proxy: undefined,
  });

  try {
    await coreConfig.initialize();
    await coreConfig.refreshAuth(core.AuthType.USE_GEMINI);

    const client = coreConfig.getGeminiClient();
    if (!client) return { text: '', error: 'GeminiClient is null' };

    const controller = new AbortController();
    const timeoutId = setTimeout(() => controller.abort(), PER_MODEL_TIMEOUT_MS);

    // For multi-turn, send the last user message (Gemini engine doesn't support
    // explicit conversation history in single sendMessageStream call).
    // Prepend prior turns as context in the prompt.
    let finalPrompt: string;
    if (prompt.messages && prompt.messages.length > 0) {
      const contextLines = prompt.messages.slice(0, -1).map((m) => `[${m.role}]: ${m.content}`);
      const lastMsg = prompt.messages[prompt.messages.length - 1];
      finalPrompt = `Previous conversation:\n${contextLines.join('\n')}\n\n[${lastMsg.role}]: ${lastMsg.content}`;
    } else {
      finalPrompt = prompt.prompt;
    }

    const stream = client.sendMessageStream(finalPrompt, controller.signal, `eval-${Date.now()}`);
    let fullResponse = '';

    try {
      for await (const event of stream) {
        if (event.type === 'content') {
          fullResponse += typeof event.value === 'string' ? event.value : '';
        } else if (event.type === 'error') {
          throw new Error(typeof event.value === 'string' ? event.value : JSON.stringify(event.value));
        }
      }
    } finally {
      clearTimeout(timeoutId);
    }

    return { text: fullResponse };
  } catch (err: any) {
    if (err.name === 'AbortError') return { text: '', error: 'timeout' };
    return { text: '', error: err.message?.slice(0, 200) };
  } finally {
    try {
      coreConfig.dispose?.();
    } catch {
      /* ignore */
    }
  }
}

// --- Run single eval ---
async function runSingleEval(core: any, config: ModelConfig, prompt: EvalPrompt): Promise<EvalResult> {
  const startTime = Date.now();

  const { text, error } =
    config.mode === 'gemini-engine'
      ? await callGeminiEngine(core, config, prompt)
      : await callOpenAIApi(config, prompt);

  const latencyMs = Date.now() - startTime;

  if (error) {
    return {
      provider: config.provider,
      model: config.model,
      promptId: prompt.id,
      status: error === 'timeout' ? 'timeout' : 'error',
      response: text.slice(0, 500),
      latencyMs,
      error,
    };
  }

  // Check keywords
  let keywordsMatched = 0;
  const keywordsTotal = prompt.expectKeywords?.length || 0;
  if (prompt.expectKeywords) {
    for (const kw of prompt.expectKeywords) {
      if (text.toLowerCase().includes(kw.toLowerCase())) keywordsMatched++;
    }
  }

  return {
    provider: config.provider,
    model: config.model,
    promptId: prompt.id,
    status: keywordsTotal === 0 || keywordsMatched > 0 ? 'pass' : 'fail',
    response: text.slice(0, 500),
    latencyMs,
    keywordsMatched,
    keywordsTotal,
  };
}

// --- Key validation ---
async function validateKey(core: any, config: ModelConfig): Promise<boolean> {
  console.log(`  Validating ${config.provider} (${config.model})...`);
  const result = await runSingleEval(core, config, {
    id: 'key-check',
    category: 'basic',
    prompt: 'Say "ok".',
    expectKeywords: [],
  });
  if (result.status === 'pass') {
    console.log(`  ✓ ${config.provider} (${result.latencyMs}ms)`);
    return true;
  }
  console.log(`  ✗ ${config.provider}: ${result.error || result.status}`);
  return false;
}

// --- Generate markdown report ---
function generateReport(session: EvalSession): string {
  const lines: string[] = [];
  lines.push('# Model Evaluation Report');
  lines.push('');
  lines.push(`**Date**: ${session.startedAt}`);
  lines.push(`**Prompt Set Version**: ${session.version}`);
  lines.push(`**Models Tested**: ${new Set(session.results.map((r) => r.provider)).size}`);
  lines.push(`**Prompts**: ${EVAL_PROMPTS.length} (T1-T10 generic + M1-M10 Margay scenarios)`);
  if (session.skippedProviders.length > 0) {
    lines.push(`**Skipped**: ${session.skippedProviders.join(', ')}`);
  }
  lines.push('');
  lines.push('## Summary');
  lines.push('');
  lines.push('| Provider | Model | Pass | Fail | Error | Timeout | Avg Latency |');
  lines.push('|----------|-------|------|------|-------|---------|-------------|');

  const byProvider = new Map<string, EvalResult[]>();
  for (const r of session.results) {
    if (!byProvider.has(r.provider)) byProvider.set(r.provider, []);
    byProvider.get(r.provider)!.push(r);
  }

  for (const [provider, results] of byProvider) {
    const pass = results.filter((r) => r.status === 'pass').length;
    const fail = results.filter((r) => r.status === 'fail').length;
    const error = results.filter((r) => r.status === 'error').length;
    const timeout = results.filter((r) => r.status === 'timeout').length;
    const avgLatency = Math.round(results.reduce((s, r) => s + r.latencyMs, 0) / results.length);
    const model = results[0]?.model || '';
    lines.push(
      `| ${provider} | \`${model}\` | ${pass} | ${fail} | ${error} | ${timeout} | ${avgLatency}ms |`,
    );
  }
  lines.push('');

  // Separate sections for T and M prompts
  lines.push('## Generic Capabilities (T1-T10)');
  lines.push('');
  for (const prompt of EVAL_PROMPTS.filter((p) => p.id.startsWith('T'))) {
    lines.push(`### ${prompt.id} (${prompt.category})`);
    lines.push('');
    const promptText = prompt.prompt || prompt.messages?.slice(-1)[0]?.content || '';
    lines.push(`> ${promptText.slice(0, 120)}${promptText.length > 120 ? '...' : ''}`);
    lines.push('');
    lines.push('| Provider | Status | Latency | Keywords | Response (truncated) |');
    lines.push('|----------|--------|---------|----------|---------------------|');

    const promptResults = session.results.filter((r) => r.promptId === prompt.id);
    for (const r of promptResults) {
      const kwInfo = r.keywordsTotal ? `${r.keywordsMatched}/${r.keywordsTotal}` : 'n/a';
      const resp = r.response.replace(/\|/g, '\\|').replace(/\n/g, ' ').slice(0, 80);
      lines.push(`| ${r.provider} | ${r.status.toUpperCase()} | ${r.latencyMs}ms | ${kwInfo} | ${resp} |`);
    }
    lines.push('');
  }

  lines.push('## Margay Scenarios (M1-M10)');
  lines.push('');
  for (const prompt of EVAL_PROMPTS.filter((p) => p.id.startsWith('M'))) {
    lines.push(`### ${prompt.id} (${prompt.category})`);
    lines.push('');
    const promptText = prompt.prompt || prompt.messages?.slice(-1)[0]?.content || '';
    lines.push(`> ${promptText.slice(0, 120)}${promptText.length > 120 ? '...' : ''}`);
    lines.push('');
    lines.push('| Provider | Status | Latency | Keywords | Response (truncated) |');
    lines.push('|----------|--------|---------|----------|---------------------|');

    const promptResults = session.results.filter((r) => r.promptId === prompt.id);
    for (const r of promptResults) {
      const kwInfo = r.keywordsTotal ? `${r.keywordsMatched}/${r.keywordsTotal}` : 'n/a';
      const resp = r.response.replace(/\|/g, '\\|').replace(/\n/g, ' ').slice(0, 80);
      lines.push(`| ${r.provider} | ${r.status.toUpperCase()} | ${r.latencyMs}ms | ${kwInfo} | ${resp} |`);
    }
    lines.push('');
  }

  return lines.join('\n');
}

// --- Main ---
async function main() {
  console.log('=== Margay Model Evaluation v2.0 ===');
  console.log(`Prompt set v${PROMPT_SET_VERSION}, ${EVAL_PROMPTS.length} prompts\n`);

  const keys = loadKeys();
  console.log(`Loaded keys: ${Object.keys(keys).join(', ')}\n`);

  // Try manifest first, fall back to inline configs
  const allConfigs = loadFromManifest(keys) || buildModelConfigs(keys);
  console.log(`\n${allConfigs.length} model configurations.\n`);

  console.log('Loading @margay/agent-core...');
  const core = await import(CORE_PATH);
  console.log('Core loaded.\n');

  // Validate keys
  console.log('--- Key Validation ---');
  const validConfigs: ModelConfig[] = [];
  const skippedProviders: string[] = [];

  for (const config of allConfigs) {
    const valid = await validateKey(core, config);
    if (valid) {
      validConfigs.push(config);
    } else {
      skippedProviders.push(`${config.provider} (${config.model})`);
    }
  }
  console.log(`\n${validConfigs.length} valid, ${skippedProviders.length} skipped.\n`);

  if (validConfigs.length === 0) {
    console.error('No valid providers. Exiting.');
    process.exit(1);
  }

  // Session (fresh start)
  const session: EvalSession = {
    version: PROMPT_SET_VERSION,
    startedAt: new Date().toISOString(),
    results: [],
    skippedProviders,
  };

  console.log('--- Running Evaluation ---\n');
  let totalCostEstimate = 0;

  for (const config of validConfigs) {
    console.log(`\n=== ${config.provider} (${config.model}) ===`);

    for (const prompt of EVAL_PROMPTS) {
      if (totalCostEstimate > COST_CAP_USD) {
        console.log(`\n  COST CAP reached. Stopping.`);
        break;
      }

      let result = await runSingleEval(core, config, prompt);

      // Retry on network error
      if (
        result.status === 'error' &&
        result.error &&
        /ECONNREFUSED|ETIMEDOUT|fetch failed|network/i.test(result.error)
      ) {
        console.log(`  ${prompt.id}: retrying (network error)...`);
        result = await runSingleEval(core, config, prompt);
      }

      session.results.push(result);
      const icon = result.status === 'pass' ? '✓' : result.status === 'fail' ? '✗' : '!';
      console.log(`  ${icon} ${prompt.id}: ${result.status} (${result.latencyMs}ms)`);

      totalCostEstimate += 0.005;
      fs.writeFileSync(RESULTS_FILE, JSON.stringify(session, null, 2));
    }

    if (totalCostEstimate > COST_CAP_USD) break;
  }

  // Generate report
  console.log('\n--- Generating Report ---');
  const report = generateReport(session);
  fs.writeFileSync(REPORT_FILE, report);
  console.log(`Report: ${REPORT_FILE}`);

  const passCount = session.results.filter((r) => r.status === 'pass').length;
  console.log(`\n=== Done: ${passCount}/${session.results.length} passed ===`);
}

main().catch((err) => {
  console.error('Fatal error:', err);
  process.exit(1);
});
