/**
 * Model Evaluation Runner v3.0
 * - Reads model-manifest.json (generated by resolve-models.ts)
 * - Falls back to inline buildModelConfigs() if no manifest exists (v1.0 compat)
 * - Supports: text, multi-turn, func-call, streaming, error test types
 * - Backends: OpenAI-compatible API, Anthropic Messages API, Gemini engine
 *
 * Usage:
 *   npx tsx scripts/model-eval/resolve-models.ts   # Run preflight first
 *   npx tsx scripts/model-eval/run-eval.ts          # Then run eval
 *
 * Reads API keys from models_test.key in project root.
 * Results written incrementally to scripts/model-eval/results.json
 * Final report generated at docs/model-eval-report.md
 */

import path from 'node:path';
import os from 'node:os';
import fs from 'node:fs';
import { EVAL_PROMPTS, PROMPT_SET_VERSION, type EvalPrompt, type ChatMessage } from './prompts';

const PROJECT_ROOT = path.resolve(__dirname, '../..');
const CORE_PATH = path.resolve(PROJECT_ROOT, 'vendor/gemini-cli/packages/core/dist/index.js');
const RESULTS_FILE = path.resolve(__dirname, 'results.json');
const REPORT_FILE = path.resolve(PROJECT_ROOT, 'docs/model-eval-report.md');
const KEY_FILE = path.resolve(PROJECT_ROOT, 'models_test.key');
const MANIFEST_FILE = path.resolve(__dirname, 'model-manifest.json');

const PER_MODEL_TIMEOUT_MS = 90_000;
const COST_CAP_USD = 20;

// Key aliases (handles typos like "calude" → "anthropic")
const KEY_ALIASES: Record<string, string> = {
  calude: 'anthropic',
  claude: 'anthropic',
};

interface ModelConfig {
  provider: string;
  model: string;
  mode: 'gemini-engine' | 'openai-api' | 'anthropic-api';
  apiKey: string;
  baseUrl: string;
}

interface EvalResult {
  provider: string;
  model: string;
  promptId: string;
  testType: string;
  status: 'pass' | 'fail' | 'error' | 'timeout' | 'skip';
  response: string;
  latencyMs: number;
  error?: string;
  keywordsMatched?: number;
  keywordsTotal?: number;
}

interface EvalSession {
  version: string;
  startedAt: string;
  results: EvalResult[];
  skippedProviders: string[];
}

// --- Parse key file ---
function loadKeys(): Record<string, string> {
  const content = fs.readFileSync(KEY_FILE, 'utf-8');
  const keys: Record<string, string> = {};
  for (const line of content.split('\n')) {
    const trimmed = line.trim();
    if (!trimmed || trimmed.startsWith('#')) continue;
    const cleanLine = trimmed.replace(/^\(.*?\)/, '').trim();
    if (!cleanLine) continue;
    const colonIdx = cleanLine.indexOf(':');
    if (colonIdx <= 0) continue;
    let provider = cleanLine.slice(0, colonIdx).trim().toLowerCase();
    const key = cleanLine.slice(colonIdx + 1).trim();
    if (KEY_ALIASES[provider]) provider = KEY_ALIASES[provider];
    if (key) keys[provider] = key;
  }
  return keys;
}

// --- Load configs from manifest (v3.0) ---
function loadFromManifest(keys: Record<string, string>): ModelConfig[] | null {
  if (!fs.existsSync(MANIFEST_FILE)) return null;

  const manifest = JSON.parse(fs.readFileSync(MANIFEST_FILE, 'utf-8'));
  const configs: ModelConfig[] = [];

  for (const entry of manifest.models) {
    if (entry.status === 'skip') continue;

    const apiKey = keys[entry.keyName];
    if (!apiKey) continue;

    configs.push({
      provider: `${entry.provider}${entry.status === 'fallback' ? ' (fb)' : ''}`,
      model: entry.resolvedModel,
      mode: entry.mode,
      apiKey,
      baseUrl: entry.baseUrl,
    });
  }

  console.log(`Loaded ${configs.length} models from manifest v${manifest.version}`);
  console.log(`  Resolved at: ${manifest.resolvedAt}`);
  const skipped = manifest.models.filter((m: any) => m.status === 'skip');
  if (skipped.length > 0) {
    console.log(
      `  Skipped: ${skipped.map((m: any) => `${m.provider} (${m.skipReason})`).join(', ')}`,
    );
  }
  return configs;
}

// --- Inline model configs (v1.0 fallback) ---
function buildModelConfigs(keys: Record<string, string>): ModelConfig[] {
  console.log('WARNING: No model-manifest.json found. Using v1.0 inline configs.');
  console.log('Run "npx tsx scripts/model-eval/resolve-models.ts" to generate manifest.\n');
  const configs: ModelConfig[] = [];
  if (keys['gemini'])
    configs.push({
      provider: 'Google',
      model: 'gemini-2.5-flash',
      mode: 'gemini-engine',
      apiKey: keys['gemini'],
      baseUrl: '',
    });
  if (keys['openai'])
    configs.push({
      provider: 'OpenAI',
      model: 'gpt-4.1-mini',
      mode: 'openai-api',
      apiKey: keys['openai'],
      baseUrl: 'https://api.openai.com/v1',
    });
  if (keys['deepseek'])
    configs.push({
      provider: 'DeepSeek',
      model: 'deepseek-chat',
      mode: 'openai-api',
      apiKey: keys['deepseek'],
      baseUrl: 'https://api.deepseek.com',
    });
  return configs;
}

// --- Build messages for a prompt ---
function buildMessages(prompt: EvalPrompt): ChatMessage[] {
  if (prompt.messages && prompt.messages.length > 0) {
    return prompt.messages;
  }
  return [{ role: 'user', content: prompt.prompt }];
}

// --- OpenAI-compatible API call ---
async function callOpenAIApi(
  config: ModelConfig,
  prompt: EvalPrompt,
): Promise<{ text: string; error?: string }> {
  const controller = new AbortController();
  const timeoutId = setTimeout(() => controller.abort(), PER_MODEL_TIMEOUT_MS);
  const messages = buildMessages(prompt);
  const isOpenAIDirect = config.baseUrl.includes('api.openai.com');

  try {
    // Build request body
    const body: Record<string, unknown> = {
      model: config.model,
      messages,
      ...(isOpenAIDirect
        ? { max_completion_tokens: prompt.maxTokens || 1024 }
        : { max_tokens: prompt.maxTokens || 1024 }),
    };

    // Add tools for func-call tests (OpenAI format)
    if (prompt.testType === 'func-call' && prompt.tools) {
      body.tools = prompt.tools.map((t) => ({
        type: 'function',
        function: {
          name: t.name,
          description: t.description,
          parameters: t.parameters,
        },
      }));
    }

    // Streaming test
    if (prompt.testType === 'streaming') {
      body.stream = true;
    }

    const resp = await fetch(`${config.baseUrl}/chat/completions`, {
      method: 'POST',
      headers: {
        'Content-Type': 'application/json',
        Authorization: `Bearer ${config.apiKey}`,
        ...(config.baseUrl.includes('openrouter')
          ? { 'HTTP-Referer': 'https://margay.app', 'X-Title': 'Margay Model Eval' }
          : {}),
      },
      body: JSON.stringify(body),
      signal: controller.signal,
    });

    if (!resp.ok) {
      const errBody = await resp.text().catch(() => '');
      // For error test, a proper error response is a "pass"
      if (prompt.testType === 'error') {
        return { text: `HTTP ${resp.status}`, error: undefined };
      }
      return { text: '', error: `HTTP ${resp.status}: ${errBody.slice(0, 200)}` };
    }

    // Streaming: collect chunks
    if (prompt.testType === 'streaming') {
      const text = await resp.text();
      const hasData = text.includes('data:');
      return { text: hasData ? 'streaming_ok' : '', error: hasData ? undefined : 'no SSE data' };
    }

    const data = (await resp.json()) as any;

    // Extract tool calls if present
    const toolCalls = data.choices?.[0]?.message?.tool_calls;
    if (toolCalls && toolCalls.length > 0) {
      const toolText = toolCalls
        .map(
          (tc: any) =>
            `${tc.function?.name}(${JSON.stringify(JSON.parse(tc.function?.arguments || '{}'))})`,
        )
        .join('\n');
      return { text: toolText };
    }

    const text = data.choices?.[0]?.message?.content || '';
    return { text };
  } catch (err: any) {
    if (err.name === 'AbortError') return { text: '', error: 'timeout' };
    return { text: '', error: err.message?.slice(0, 200) };
  } finally {
    clearTimeout(timeoutId);
  }
}

// --- Anthropic Messages API call ---
async function callAnthropicApi(
  config: ModelConfig,
  prompt: EvalPrompt,
): Promise<{ text: string; error?: string }> {
  const controller = new AbortController();
  const timeoutId = setTimeout(() => controller.abort(), PER_MODEL_TIMEOUT_MS);
  const messages = buildMessages(prompt);

  try {
    const body: Record<string, unknown> = {
      model: config.model,
      max_tokens: prompt.maxTokens || 1024,
      messages,
    };

    // Add tools for func-call tests (Anthropic format)
    if (prompt.testType === 'func-call' && prompt.tools) {
      body.tools = prompt.tools.map((t) => ({
        name: t.name,
        description: t.description,
        input_schema: t.parameters,
      }));
    }

    // Streaming test
    if (prompt.testType === 'streaming') {
      body.stream = true;
    }

    const resp = await fetch('https://api.anthropic.com/v1/messages', {
      method: 'POST',
      headers: {
        'Content-Type': 'application/json',
        'x-api-key': config.apiKey,
        'anthropic-version': '2023-06-01',
      },
      body: JSON.stringify(body),
      signal: controller.signal,
    });

    if (!resp.ok) {
      const errBody = await resp.text().catch(() => '');
      if (prompt.testType === 'error') {
        return { text: `HTTP ${resp.status}`, error: undefined };
      }
      return { text: '', error: `HTTP ${resp.status}: ${errBody.slice(0, 200)}` };
    }

    // Streaming
    if (prompt.testType === 'streaming') {
      const text = await resp.text();
      const hasEvents = text.includes('event:');
      return {
        text: hasEvents ? 'streaming_ok' : '',
        error: hasEvents ? undefined : 'no SSE events',
      };
    }

    const data = (await resp.json()) as any;
    const parts: string[] = [];
    for (const block of data.content || []) {
      if (block.type === 'text') {
        parts.push(block.text);
      } else if (block.type === 'tool_use') {
        parts.push(`${block.name}(${JSON.stringify(block.input)})`);
      }
    }
    return { text: parts.join('\n') };
  } catch (err: any) {
    if (err.name === 'AbortError') return { text: '', error: 'timeout' };
    return { text: '', error: err.message?.slice(0, 200) };
  } finally {
    clearTimeout(timeoutId);
  }
}

// --- Gemini engine call ---
async function callGeminiEngine(
  core: any,
  config: ModelConfig,
  prompt: EvalPrompt,
): Promise<{ text: string; error?: string }> {
  for (const k of [
    'GEMINI_API_KEY',
    'GOOGLE_GEMINI_BASE_URL',
    'OPENAI_API_KEY',
    'OPENAI_BASE_URL',
  ]) {
    delete process.env[k];
  }
  process.env.GEMINI_API_KEY = config.apiKey;

  const workspace = os.tmpdir();
  const coreConfig = new core.Config({
    sessionId: `eval-${Date.now()}`,
    targetDir: workspace,
    cwd: workspace,
    debugMode: false,
    model: config.model,
    usageStatisticsEnabled: false,
    proxy: undefined,
  });

  try {
    await coreConfig.initialize();
    await coreConfig.refreshAuth(core.AuthType.USE_GEMINI);

    const client = coreConfig.getGeminiClient();
    if (!client) return { text: '', error: 'GeminiClient is null' };

    const controller = new AbortController();
    const timeoutId = setTimeout(() => controller.abort(), PER_MODEL_TIMEOUT_MS);

    // Build prompt text (serialize multi-turn as context prefix)
    let finalPrompt: string;
    if (prompt.messages && prompt.messages.length > 0) {
      const contextLines = prompt.messages.slice(0, -1).map((m) => `[${m.role}]: ${m.content}`);
      const lastMsg = prompt.messages[prompt.messages.length - 1];
      finalPrompt = `Previous conversation:\n${contextLines.join('\n')}\n\n[${lastMsg.role}]: ${lastMsg.content}`;
    } else {
      finalPrompt = prompt.prompt;
    }

    const stream = client.sendMessageStream(finalPrompt, controller.signal, `eval-${Date.now()}`);
    let fullResponse = '';

    try {
      for await (const event of stream) {
        if (event.type === 'content') {
          fullResponse += typeof event.value === 'string' ? event.value : '';
        } else if (event.type === 'error') {
          throw new Error(
            typeof event.value === 'string' ? event.value : JSON.stringify(event.value),
          );
        }
      }
    } finally {
      clearTimeout(timeoutId);
    }

    // Gemini engine inherently streams via sendMessageStream.
    // If we got content, streaming works — return sentinel for scoring.
    if (prompt.testType === 'streaming') {
      return { text: fullResponse ? 'streaming_ok' : '', error: fullResponse ? undefined : 'empty stream' };
    }

    return { text: fullResponse };
  } catch (err: any) {
    if (err.name === 'AbortError') return { text: '', error: 'timeout' };
    return { text: '', error: err.message?.slice(0, 200) };
  } finally {
    try {
      coreConfig.dispose?.();
    } catch {
      /* ignore */
    }
  }
}

// --- Dispatch call by mode ---
async function callModel(
  core: any,
  config: ModelConfig,
  prompt: EvalPrompt,
): Promise<{ text: string; error?: string }> {
  switch (config.mode) {
    case 'gemini-engine':
      return callGeminiEngine(core, config, prompt);
    case 'anthropic-api':
      return callAnthropicApi(config, prompt);
    case 'openai-api':
      return callOpenAIApi(config, prompt);
  }
}

// --- Run single eval ---
async function runSingleEval(core: any, config: ModelConfig, prompt: EvalPrompt): Promise<EvalResult> {
  const startTime = Date.now();
  const testType = prompt.testType || 'text';

  // Error test: send intentionally bad request (empty messages)
  let evalPrompt = prompt;
  if (testType === 'error') {
    evalPrompt = {
      ...prompt,
      prompt: 'x'.repeat(1_000_000), // Exceed context limit
      messages: undefined,
    };
  }

  const { text, error } = await callModel(core, config, evalPrompt);
  const latencyMs = Date.now() - startTime;

  // Error test: success if the backend properly rejected the bad input.
  // HTTP errors (OpenAI/Anthropic) or SDK runtime errors (Gemini) are both valid rejections.
  // Excluded: timeout and transient network failures — these indicate connectivity issues,
  // not the backend's ability to reject invalid input.
  if (testType === 'error') {
    const isNetworkFailure = !!error && /ECONNREFUSED|ETIMEDOUT|fetch failed|network|timeout/i.test(error);
    const gotProperError =
      text.startsWith('HTTP') || (!!error && !isNetworkFailure);
    return {
      provider: config.provider,
      model: config.model,
      promptId: prompt.id,
      testType,
      status: gotProperError ? 'pass' : 'fail',
      response: (text || error || '').slice(0, 500),
      latencyMs,
    };
  }

  // Streaming test: success if we got streaming data
  if (testType === 'streaming') {
    return {
      provider: config.provider,
      model: config.model,
      promptId: prompt.id,
      testType,
      status: text === 'streaming_ok' ? 'pass' : 'fail',
      response: text.slice(0, 500),
      latencyMs,
      error,
    };
  }

  if (error) {
    return {
      provider: config.provider,
      model: config.model,
      promptId: prompt.id,
      testType,
      status: error === 'timeout' ? 'timeout' : 'error',
      response: text.slice(0, 500),
      latencyMs,
      error,
    };
  }

  // Check keywords
  let keywordsMatched = 0;
  const keywordsTotal = prompt.expectKeywords?.length || 0;
  if (prompt.expectKeywords) {
    for (const kw of prompt.expectKeywords) {
      if (text.toLowerCase().includes(kw.toLowerCase())) keywordsMatched++;
    }
  }

  return {
    provider: config.provider,
    model: config.model,
    promptId: prompt.id,
    testType,
    status: keywordsTotal === 0 || keywordsMatched > 0 ? 'pass' : 'fail',
    response: text.slice(0, 500),
    latencyMs,
    keywordsMatched,
    keywordsTotal,
  };
}

// --- Key validation ---
async function validateKey(core: any, config: ModelConfig): Promise<boolean> {
  console.log(`  Validating ${config.provider} (${config.model})...`);
  const result = await runSingleEval(core, config, {
    id: 'key-check',
    category: 'basic',
    prompt: 'Say "ok".',
    expectKeywords: [],
  });
  if (result.status === 'pass') {
    console.log(`  ✓ ${config.provider} (${result.latencyMs}ms)`);
    return true;
  }
  console.log(`  ✗ ${config.provider}: ${result.error || result.status}`);
  return false;
}

// --- Generate markdown report ---
function generateReport(session: EvalSession): string {
  const lines: string[] = [];
  lines.push('# Model Evaluation Report v3.0');
  lines.push('');
  lines.push(`**Date**: ${session.startedAt}`);
  lines.push(`**Prompt Set Version**: ${session.version}`);
  lines.push(`**Models Tested**: ${new Set(session.results.map((r) => r.provider)).size}`);
  lines.push(
    `**Prompts**: ${EVAL_PROMPTS.length} (T1-T18 plan-aligned + M1-M10 Margay scenarios)`,
  );
  if (session.skippedProviders.length > 0) {
    lines.push(`**Skipped**: ${session.skippedProviders.join(', ')}`);
  }
  lines.push('');

  // Summary table
  lines.push('## Summary');
  lines.push('');
  lines.push('| Provider | Model | Pass | Fail | Error | Timeout | Avg Latency |');
  lines.push('|----------|-------|------|------|-------|---------|-------------|');

  const byProvider = new Map<string, EvalResult[]>();
  for (const r of session.results) {
    if (!byProvider.has(r.provider)) byProvider.set(r.provider, []);
    byProvider.get(r.provider)!.push(r);
  }

  for (const [provider, results] of byProvider) {
    const pass = results.filter((r) => r.status === 'pass').length;
    const fail = results.filter((r) => r.status === 'fail').length;
    const error = results.filter((r) => r.status === 'error').length;
    const timeout = results.filter((r) => r.status === 'timeout').length;
    const avgLatency = Math.round(results.reduce((s, r) => s + r.latencyMs, 0) / results.length);
    const model = results[0]?.model || '';
    lines.push(
      `| ${provider} | \`${model}\` | ${pass} | ${fail} | ${error} | ${timeout} | ${avgLatency}ms |`,
    );
  }
  lines.push('');

  // Detailed results by category
  // Use 'Tx-' prefix (with dash) so T1- doesn't match T10-, T11-, etc.
  const categories = [
    { label: 'Basic & Instruction (T1-T2)', filter: (p: EvalPrompt) => ['T1-', 'T2-'].some((t) => p.id.startsWith(t)) },
    { label: 'Code (T3, T11)', filter: (p: EvalPrompt) => ['T3-', 'T11-'].some((t) => p.id.startsWith(t)) },
    { label: 'Reasoning (T4, T18)', filter: (p: EvalPrompt) => ['T4-', 'T18-'].some((t) => p.id.startsWith(t)) },
    { label: 'Function Calling (T5, T15)', filter: (p: EvalPrompt) => ['T5-', 'T15-'].some((t) => p.id.startsWith(t)) },
    { label: 'Multi-Turn & Creative (T6-T8)', filter: (p: EvalPrompt) => ['T6-', 'T7-', 'T8-'].some((t) => p.id.startsWith(t)) },
    { label: 'Practical Tasks (T9-T10, T12-T14)', filter: (p: EvalPrompt) => ['T9-', 'T10-', 'T12-', 'T13-', 'T14-'].some((t) => p.id.startsWith(t)) },
    { label: 'Compatibility (T16-T17)', filter: (p: EvalPrompt) => ['T16-', 'T17-'].some((t) => p.id.startsWith(t)) },
    { label: 'Margay Scenarios (M1-M10)', filter: (p: EvalPrompt) => p.id.startsWith('M') },
  ];

  for (const cat of categories) {
    const prompts = EVAL_PROMPTS.filter(cat.filter);
    if (prompts.length === 0) continue;

    lines.push(`## ${cat.label}`);
    lines.push('');

    for (const prompt of prompts) {
      lines.push(`### ${prompt.id} (${prompt.category})`);
      lines.push('');
      const promptText = prompt.prompt || prompt.messages?.slice(-1)[0]?.content || '';
      lines.push(`> ${promptText.slice(0, 120)}${promptText.length > 120 ? '...' : ''}`);
      lines.push('');
      lines.push('| Provider | Status | Latency | Keywords | Response (truncated) |');
      lines.push('|----------|--------|---------|----------|---------------------|');

      const promptResults = session.results.filter((r) => r.promptId === prompt.id);
      for (const r of promptResults) {
        const kwInfo = r.keywordsTotal ? `${r.keywordsMatched}/${r.keywordsTotal}` : 'n/a';
        const resp = r.response.replace(/\|/g, '\\|').replace(/\n/g, ' ').slice(0, 80);
        lines.push(
          `| ${r.provider} | ${r.status.toUpperCase()} | ${r.latencyMs}ms | ${kwInfo} | ${resp} |`,
        );
      }
      lines.push('');
    }
  }

  return lines.join('\n');
}

// --- Main ---
async function main() {
  console.log('=== Margay Model Evaluation v3.0 ===');
  console.log(`Prompt set v${PROMPT_SET_VERSION}, ${EVAL_PROMPTS.length} prompts\n`);

  const keys = loadKeys();
  console.log(`Loaded keys: ${Object.keys(keys).join(', ')}\n`);

  // Try manifest first, fall back to inline configs
  const allConfigs = loadFromManifest(keys) || buildModelConfigs(keys);
  console.log(`\n${allConfigs.length} model configurations.\n`);

  console.log('Loading @margay/agent-core...');
  const core = await import(CORE_PATH);
  console.log('Core loaded.\n');

  // Validate keys
  console.log('--- Key Validation ---');
  const validConfigs: ModelConfig[] = [];
  const skippedProviders: string[] = [];

  for (const config of allConfigs) {
    const valid = await validateKey(core, config);
    if (valid) {
      validConfigs.push(config);
    } else {
      skippedProviders.push(`${config.provider} (${config.model})`);
    }
  }
  console.log(`\n${validConfigs.length} valid, ${skippedProviders.length} skipped.\n`);

  if (validConfigs.length === 0) {
    console.error('No valid providers. Exiting.');
    process.exit(1);
  }

  // Session
  const session: EvalSession = {
    version: PROMPT_SET_VERSION,
    startedAt: new Date().toISOString(),
    results: [],
    skippedProviders,
  };

  console.log('--- Running Evaluation ---\n');
  let totalCostEstimate = 0;

  for (const config of validConfigs) {
    console.log(`\n=== ${config.provider} (${config.model}) ===`);

    for (const prompt of EVAL_PROMPTS) {
      if (totalCostEstimate > COST_CAP_USD) {
        console.log(`\n  COST CAP reached ($${COST_CAP_USD}). Stopping.`);
        break;
      }

      let result = await runSingleEval(core, config, prompt);

      // Retry on network error
      if (
        result.status === 'error' &&
        result.error &&
        /ECONNREFUSED|ETIMEDOUT|fetch failed|network/i.test(result.error)
      ) {
        console.log(`  ${prompt.id}: retrying (network error)...`);
        result = await runSingleEval(core, config, prompt);
      }

      session.results.push(result);
      const icon = result.status === 'pass' ? '✓' : result.status === 'fail' ? '✗' : '!';
      console.log(`  ${icon} ${prompt.id}: ${result.status} (${result.latencyMs}ms)`);

      totalCostEstimate += 0.01; // conservative per-call estimate
      fs.writeFileSync(RESULTS_FILE, JSON.stringify(session, null, 2));
    }

    if (totalCostEstimate > COST_CAP_USD) break;
  }

  // Generate report
  console.log('\n--- Generating Report ---');
  const report = generateReport(session);
  fs.writeFileSync(REPORT_FILE, report);
  console.log(`Report: ${REPORT_FILE}`);

  const passCount = session.results.filter((r) => r.status === 'pass').length;
  console.log(`\n=== Done: ${passCount}/${session.results.length} passed ===`);
}

main().catch((err) => {
  console.error('Fatal error:', err);
  process.exit(1);
});
